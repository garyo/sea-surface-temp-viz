name: Create Images and Make Release

on:
  workflow_dispatch:
  schedule:
    # Data is updated at 9am EDT = 13:00 hrs UTC, so run just after that
    # min hr dom mo dow
    - cron: '15 13 * * *'
jobs:
  make-images:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    # Use "uv" for python and dependency management
    - uses: astral-sh/setup-uv@v2
      with:
        enable-cache: true
    - name: Install dependencies
      run: uv sync
    - name: Generate graphs
      run: |
        uv run prune-cache.py --inplace --days 90 # Re-fetch most recent 90 days, in case of reanalysis
        mkdir -p ./output
        uv run sea-surface-temps.py --mode graph --dataset sst --out ./output/sst-all.png
        uv run sea-surface-temps.py --mode graph --dataset anom --out ./output/sst-all-anom.png
        uv run sea-surface-temps.py --mode graph --dataset sst --out ./output/sst-all.svg
        uv run sea-surface-temps.py --mode graph --dataset anom --out ./output/sst-all-anom.svg

    # Maps: OK if these fail; we're looking for yesterday, might not be there yet.
    - name: Generate maps
      run: |
        mkdir -p ./output
        uv run sea-surface-temps.py --mode map --dataset sst --days-ago 2 --out ./output || true
        uv run sea-surface-temps.py --mode map --dataset anom --days-ago 2 --out ./output || true

    # This generates textures (.webp) and associated colormap json files (-metadata.json)
    # Generate for the last 7 days to build up a time series (and catch any missed days)
    - name: Generate map textures
      run: |
        rm -rf ./maps
        mkdir -p ./maps
        for days_ago in 2 3 4 5 6 7 8; do
          echo "Generating textures for $days_ago days ago..."
          uv run sea-surface-temps.py --mode texture --dataset sst --days-ago $days_ago --out ./maps || true
          uv run sea-surface-temps.py --mode texture --dataset anom --days-ago $days_ago --out ./maps || true
        done

    # Note: index.json will be generated by upload-to-s3.py from S3 contents
    # This ensures it includes all dates, not just the ones from this run

    - name: Get current date
      id: date
      run: echo "date=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

    - name: Create Release
      id: create_release
      uses: softprops/action-gh-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        name: ${{ steps.date.outputs.date }}
        tag_name: ${{ steps.date.outputs.date }}
        draft: false
        prerelease: false
        body: "Latest plots as of ${{ steps.date.outputs.date }}"
        files: |
           output/sst-sst-map.webp
           output/sst-anom-map.webp
           output/sst-all.png
           output/sst-all-anom.png
           output/sst-all.svg
           output/sst-all-anom.svg
           maps/sst-temp-equirect.webp
           maps/sst-temp-equirect-metadata.json
           maps/sst-temp-anomaly-equirect.webp
           maps/sst-temp-anomaly-equirect-metadata.json
           maps/*-sst-temp-equirect.webp
           maps/*-sst-temp-equirect-metadata.json
           maps/*-sst-temp-anomaly-equirect.webp
           maps/*-sst-temp-anomaly-equirect-metadata.json
           maps/index.json
                      
    - name: Upload files to S3 and generate index
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        # upload-to-s3.py will:
        # 1. Upload all files from ./maps
        # 2. Query S3 to get all dated files (including previous uploads)
        # 3. Generate and upload index.json
        uv run ./upload-to-s3.py --maps-dir ./maps

